from flask import Blueprint, request, jsonify
import os
import joblib
import requests
from sentence_transformers import SentenceTransformer

bp = Blueprint('mbert', __name__, url_prefix='/mbert')

@bp.route('/build', methods=['POST'])
def build_bert():
    try:
        print("ğŸ“¥ Received request to /mbert/build")

        data = request.json
        print(f"ğŸ“¦ Request data: {data}")

        dataset_id = data.get('dataset_id')
        table_name = data.get('table_name', 'documents')

        if not dataset_id or table_name not in ['documents', 'queries']:
            print("âŒ Invalid dataset_id or table_name")
            return jsonify({"error": "Missing or invalid 'dataset_id' or 'table_name'"}), 400

        # ğŸ” Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©
        print("ğŸ”„ Sending request to preprocessing service...")
        preprocess_url = "http://127.0.0.1:5000/preprocess/bulk"
        response = requests.post(preprocess_url, json={
            "dataset_id": dataset_id,
            "table_name": table_name,
            "options": {
                "normalize": True,
                "spell_correction": False,
                "process_dates": False,
                "tokenize": False,
                "remove_stopwords": False,
                "lemmatize": False,
                "stem": False
            }
        })

        print(f"ğŸ§¾ Preprocessing response status: {response.status_code}")

        if response.status_code != 200:
            print("âŒ Failed to preprocess data")
            return jsonify({"error": "Failed to preprocess data", "details": response.text}), 500

        result = response.json()
        processed_data = result.get("processed_data", {})

        if not processed_data:
            print("âŒ No processed data returned")
            return jsonify({"error": "No processed data returned"}), 404

        total_docs = len(processed_data)
        print(f"ğŸš€ Starting BERT encoding for {total_docs} {table_name} in dataset {dataset_id}")

        # ğŸ§  ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª
        print("ğŸ“¥ Loading multilingual BERT model...")
        model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
        print("âœ… Model loaded")

        doc_vectors = {}
        for idx, (doc_id, tokens) in enumerate(processed_data.items(), 1):
            text = " ".join(tokens)
            doc_vectors[doc_id] = model.encode(text, convert_to_numpy=True)

            if idx % 100 == 0 or idx == total_docs:
                print(f"ğŸ“Š Encoded {idx} / {total_docs} {table_name}")

        print(f"ğŸ‰ Finished encoding all {total_docs} {table_name}")

        # ğŸ’¾ Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª
        model_dir = f"data/mbert/{table_name}_{dataset_id}"
        print(f"ğŸ’¾ Saving vectors to {model_dir}")
        os.makedirs(model_dir, exist_ok=True)
        joblib.dump(doc_vectors, os.path.join(model_dir, "doc_vectors.pkl"))
        print("âœ… Saved doc_vectors.pkl")

        model.save(os.path.join(model_dir, "model"))
        print("âœ… Model saved")

        return jsonify({
            "message": f"âœ… Multilingual BERT vectors built for {table_name} of dataset {dataset_id}",
            "num_documents": total_docs
        })

    except Exception as e:
        print("âŒ Error during BERT vector building:", e)
        return jsonify({"error": str(e)}), 500
