from flask import Blueprint, request, jsonify
import os
import joblib
import requests
from sentence_transformers import SentenceTransformer
from app.database.models import get_documents
import faiss
import numpy as np
import time

model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')
bp = Blueprint('vector_store_', __name__, url_prefix='/vector_store')

@bp.route('/match_query', methods=['POST'])
def match_user_query():
    try:
        start = time.time()  # â±ï¸ Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙƒÙ„ÙŠ

        data = request.json
        dataset_id = data.get('dataset_id')
        query_text = data.get('text')

        print("ğŸ“¥ Received request with dataset_id:", dataset_id)
        print("ğŸ“ Original query text:", query_text)

        if not dataset_id or not query_text:
            return jsonify({"error": "Missing 'dataset_id' or 'text'"}), 400

        # ğŸ” 1. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        preprocess_url = "http://127.0.0.1:5000/preprocess/query"
        print("ğŸ” Sending request to:", preprocess_url)
        response = requests.post(preprocess_url, json={"text": query_text})
        if response.status_code != 200:
            return jsonify({"error": "Failed to preprocess query"}), 500

        tokens = response.json().get("tokens")
        print("ğŸ”¤ Received tokens:", tokens)

        if not tokens:
            return jsonify({"error": "No tokens returned from preprocess"}), 500

        # ğŸ§  2. ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙƒÙ€ vector Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT
        print("ğŸ§  Encoding query with BERT model...")
        query_vector = model.encode(query_text, convert_to_numpy=True).astype('float32').reshape(1, -1)
        print("âœ… Query vector shape:", query_vector.shape)

        # ğŸ“¦ 3. ØªØ­Ù…ÙŠÙ„ ÙÙ‡Ø±Ø³ FAISS Ùˆ doc_ids
        model_dir = f"data/bert/documents_{dataset_id}"
        index_path = os.path.join(model_dir, "faiss.index")
        doc_ids_path = os.path.join(model_dir, "doc_ids.pkl")

        print("ğŸ“‚ Loading FAISS index from:", index_path)
        if not os.path.exists(index_path) or not os.path.exists(doc_ids_path):
            return jsonify({"error": "FAISS index or doc_ids not found"}), 404

        index = faiss.read_index(index_path)
        print("ğŸ“Œ FAISS index type:", type(index))

        doc_ids = joblib.load(doc_ids_path)
        print("ğŸ†” Loaded", len(doc_ids), "doc IDs")

        # ğŸ“„ 4. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        print("ğŸ—ƒï¸ Fetching documents from database...")
        all_docs = get_documents(dataset_id)
        doc_text_map = {str(doc[0]): doc[1] for doc in all_docs}
        print("ğŸ“„ Total documents in DB:", len(doc_text_map))

        # ğŸ” 5. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù‚Ø±Ø¨ 10 ÙˆØ«Ø§Ø¦Ù‚
        top_k = 10
        print("ğŸ” Searching for top", top_k, "similar documents...")
        distances, indices = index.search(query_vector, top_k)

        results = []
        for i in range(top_k):
            doc_idx = indices[0][i]
            results.append({
                "doc_id": doc_ids[doc_idx],
                "score": float(distances[0][i]),
                "text": doc_text_map.get(doc_ids[doc_idx], "")
            })

        print("âœ… Search complete. Found", len(results), "matches.")
        print("â±ï¸ Total time:", round(time.time() - start, 2), "sec")

        return jsonify({
            "query_tokens": tokens,
            "top_matches": results
        })

    except Exception as e:
        print("âŒ Exception occurred:", str(e))
        return jsonify({"error": str(e)}), 500
